---
title: "Beyond python"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    theme: beige
    logo: images/fluxim.png
    footer: <https://fluxim.com>
resources:
  - demo.pdf
---

## Why python
:::: {.columns}

::: {.column width="50%"}
* easy to use
* Read-Evaluation-Print-Loop
* large ecosystem (pandas, ML learning..)
* notebooks
:::

::: {.column width="50%"}
![](images/python-jupyter.png){height="400"}
:::

::::


## Why not (pure) python
![](images/python-interpreter.png){height="400" fig-align="center"}

[Pure python is extremely slow.]{style="color: Crimson;"}

## Test example

Try to solve boundary-value problem (BVP):

$$
\begin{aligned}
y''(x) - 2 y(x)^3 + 6 y(x) + 2 x^3 &=0 \\
y(1) &= 2 \\
y(2) &= \frac{5}{2}
\end{aligned}
$$

Analytical solution:

$$
y(x) = x + \frac{1}{x}
$$

::: aside
Example taken from [here](https://services.math.duke.edu/~jtwong/math563-2020/lectures/Lec9-BVPs.pdf).
:::

## Discretization
Define mesh $[x_0, x_1, \ldots, x_{N-1}]$ with $y_i = y(x_i)$.
$$
\begin{aligned}
h^{-2} (y_{i+1} - 2 y_i + y_{i-1}) + 6 y_i - 2 y_i^3 + 2 x_i^3 &= 0 \\
y_0 - 2 &= 0 \\
y_{N-1} - \frac{5}{2} &= 0
\end{aligned}
$$

Non-linear coupled equations for $\boldsymbol{F}(y_0, \ldots, y_{N-1}) = \{F_0, \ldots, F_{N-1}\} = 0$.

## Newton solver

0. Compute initial guess $\boldsymbol{y}^{(0)}$.
1. Linearize system. $J=\partial F_i/\partial y_j$
2. Solve linear system $J \Delta \boldsymbol{y} = -\boldsymbol{F}$.
3. Apply step: $\boldsymbol{y}^{(n+1)} = \boldsymbol{y}^{(n)} + \Delta \boldsymbol{y}$. 
4. Go back to 1.

[Step 2. usually takes the longest (LU solver)]{style="color: DarkRed;"}

## Implementation in vanilla python

[(I didn't do that)]{style="color: Green;"}

# Numpy

## Anatomy of a numpy call

```{mermaid}
%% picture was generated by Phind AI.

%%| fig-width: 8
flowchart TD
    classDef python fill:#4B8BBE,color:#fff,stroke:#306998
    classDef c fill:#A52A2A,color:#fff,stroke:#8B0000
    classDef lib fill:#228B22,color:#fff,stroke:#006400
    
    subgraph Python["Python Layer"]
        NP[NumPy Array]:::python
        API[NumPy C-API]:::python
    end
    
    subgraph C["C Interface Layer"]
        WRAP[Wrapper Functions]:::c
        CONF[Configuration]:::c
    end
    
    subgraph LIBS["Library Layer"]
        MKL[MKL Library]:::lib
        BLAS[BLAS Implementation]:::lib
        LAPACK[LAPACK Implementation]:::lib
    end
    
    NP -->|"Array Operation"| API
    API -->|"Function Call"| WRAP
    WRAP -->|"Dispatch"| CONF
    CONF -->|"Optimized Path"| MKL
    CONF -.->|"Fallback Path"| BLAS
    CONF -.->|"Fallback Path"| LAPACK
```

## Vectorization

```{.python}
a = np.array([1,2,3,4])
b = np.array([5,6,7,8])

# slow - executed in python VM
for i in range(4):
   a[i] += b[i]

# fast - executed in C library
a += b
```

Broadcasting, `einsum` etc.

## F (target function)
```{.python}
def getF(x,y):
   n = x.shape[0]
   h = 1./float(n)
   F = np.zeros(n, dtype = floatType)

   # f''
   F[1:-1] = (y[2:] + y[:-2] - 2* y[1:-1])/h**2

   F[1:-1] += 2*x[1:-1]**3
   F[1:-1] += -2*y[1:-1]**3
   F[1:-1] += 6*y[1:-1]

   F[0] = y[0] - 2
   F[-1] = y[-1] - 2.5

   return F
```


## Jacobian (linearization of discretized equation)

```{.python}
def getJacobian(x, y):
   n = x.shape[0]
   h = 1./float(n)
   jac = np.zeros((n,n), dtype = floatType)
   innermaindiag = (np.ogrid[1:n-1:1], np.ogrid[1:n-1:1])
   innerlowerdiag = (np.ogrid[1:n-1:1], np.ogrid[0:n-2:1])
   innerupperdiag = (np.ogrid[1:n-1:1], np.ogrid[2:n:1])

   jac[innerlowerdiag] = 1/h**2
   jac[innerupperdiag] = 1/h**2

   jac[innermaindiag] = -2/h**2
   jac[innermaindiag] += -6*y[1:-1]**2
   jac[innermaindiag] += 6

   jac[0,0] = 1
   jac[n-1, n-1] = 1

   return jac
```

## Main loop

```{.python}
def newtonIt(x, y, damping = 0.1):
   F = getF(x,y)
   jac = getJacobian(x,y)
   delta = np.linalg.solve(jac, -F)
   
   return y+damping*delta
```

Performance is already quite good ($0.5\ \mathrm{s}$ for our problem).

# JAX

## Open XLA (Accelerated Linear Algebra)

![](images/openxla.png){height="550" fig-align="center"}

## Example of `StableHLO`
```
func.func @main(
  %image: tensor<28x28xf32>,
  %weights: tensor<784x10xf32>,
  %bias: tensor<1x10xf32>
) -> tensor<1x10xf32> {
  %0 = "stablehlo.reshape"(%image) : (tensor<28x28xf32>) -> tensor<1x784xf32>
  %1 = "stablehlo.dot"(%0, %weights) : (tensor<1x784xf32>, tensor<784x10xf32>) -> tensor<1x10xf32>
  %2 = "stablehlo.add"(%1, %bias) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
  %3 = "stablehlo.constant"() {value = dense<0.0> : tensor<1x10xf32>} : () -> tensor<1x10xf32>
  %4 = "stablehlo.maximum"(%2, %3) : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<1x10xf32>
  "func.return"(%4): (tensor<1x10xf32>) -> ()
}
```

::: aside
Example taken from [Stable HLO documentation](https://github.com/openxla/stablehlo/blob/main/docs/spec.md).
:::


## Anatomy of a JAX call
```{mermaid}
%%| fig-height: 6
sequenceDiagram
    participant P as Python Code
    participant J as JAX JIT
    participant X as XLA Compiler
    participant C as Compiled Code
    participant E as Execution Engine

    Note over P,E: First Call (Compilation Phase)
    P->>J: Call jax.jit(function)(args)
    J->>P: Wrap arguments in tracers
    P->>J: Execute Python implementation
    J->>J: Record operations
    J->>X: Generate XLA code
    X->>C: Compile optimized code
    C->>E: Load compiled function
    E->>P: Return result

    Note over P,E: Subsequent Calls (Execution Phase)
    P->>J: Call jax.jit(function)(args)
    J->>C: Check cache
    C->>E: Use cached compiled code
    E->>P: Return result
```

## Jacobian
```{.python}
def getJacobianJ(x, y):
   n = x.shape[0]
   h = 1./float(n)*np.ones(n-2)
   innermaindiagfpp = jnp.concatenate((jnp.zeros(1), -2/h**2, jnp.zeros(1)))
   innermaindiagy3 = jnp.concatenate((jnp.zeros(1), -6*y[1:-1]**2 + 6, jnp.zeros(1)))

   innermaindiag = innermaindiagfpp + innermaindiagy3

   jacinner = jnp.diag(innermaindiag)

   innerlowerdiag = jnp.concatenate((1/h**2, jnp.zeros(1)))
   jaclower = jnp.diag(innerlowerdiag, k=-1)

   innerupperdiag = jnp.concatenate((jnp.zeros(1), 1/h**2))
   jacupper = jnp.diag(innerupperdiag, k=+1)

   jacbcdiag = jnp.concatenate((jnp.ones(1), jnp.zeros(n-2), jnp.ones(1)))
   jacbc = jnp.diag(jacbcdiag)

   jac = jacinner + jaclower + jacupper + jacbc

   return jac
```

## Main differences
* It can only JIT expressions that are calls into `jax`
* Replace `numpy.<...>` with `jax.numpy.<...>`
* Arrays are immutable
* flow statements need to rewritten

```{.python}
# Instead of
val = init_val
while cond_fun(val):
  val = body_fun(val)

# write
jax.while_loop(cond_fun, body_fun, init_val0
```

## Results
* Code can become awkward (need to get used to it)
* Only 32 bit precision (64 bit is experimental)
* Our solver does not converge fully (might be my fault)
* Execution time is $0.08\ \mathrm{s}$
* Can in principle target multiple architectures (GPU, TPU, etc.)


